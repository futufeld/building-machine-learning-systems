# Storing Data within Machine Learning Systems

## Data Management: Principles and Practices

In the 1970s, powerful techniques to precisely define the structure of data, represent relationships between that data, and even reason about operations on that data using relational algebra emerged. Today, these techniques are implemented in tools such as relational database management systems (RDBMSs) and the Structured Query Language (SQL). The use of RDBMSs and SQL is common in software systems due to their effectiveness in tasks such as efficient data storage, retrieval and management - all tasks that are essential to effective machine learning (ML) system design. However, the principles of sound data management can be supplemented with a set of practices to further increase the value of data within ML systems; practices that can assist data traceability and help guard against threats to data integrity. This blog post outlines the value that structured data can provide to ML systems and describes practices for optimising the value of data for such systems.

## Storing Structured Data

Defining the structure of domain data is immensely important in system design for many reasons. First, it forces the system designer to anticipate the logical parts of which the data consists, which ultimately determines what the system will be able to achieve using the data. Second, it forces the designer to identify the type of each logical part of the data and thus its technical representation. Third, understanding the logical and technical structure of the domain data enables the constraints, invariants and limits of the data to be understood. Fourth, the structure provides direction for the design and implementation of functionality that uses that data. Finally, an unambigous definition of data is useful simply for communicating its structure to other engineers. For these reasons, structuring data is essential regardless of whether the data will be stored.

RDBMSs exploit the structure of data to provide many advantages such as efficient data storage and indexing. Efficient storage is generally useful for any systems that store a large volume of data, of which ML systems are one class. Indexing leverages data structures to enable the fast retrieval of specific elements within the database, including interesting or problematic examples of the dataset. SQL enables the retrieval of specific subsets of the data stored within the database, making the creation of datasets that follow specific criteria possible. SQL has a straightforward, declarative syntax that concisely specifies the criteria data must meet to be present in the resultant datasets. It is useful for exploring how datasets with specific characteristics affect model training, as well as communicating how specific datasets were constructed.

RDBMSs also manage user interaction with databases, enabling them to include features such as:

* **Access control:** Access to the data within a database can be controlled, reducing the risk that the data will be accessed and misused by unauthorised individuals. This is also important for ensuring that the confidentiality and privacy of the individuals identified in domain data is maintained.
* **Shared access:** RDBMSs typically run as servers, making shared access to large datasets possible. This also enables a dedicated machine to be used for data serving tasks, freeing up other machines to perform ML tasks.
* **Granular permissions:** The permission to perform certain operations, such as the ability to modify data, can be controlled on a per-user basis. Functionality that need only consume from the database, such as model training functionality, can be limited to a read-only role, eliminating the risk that any data modified during training could be persisted back into the database in its modified form.


* **Streaming:** Data can be streamed from databases, allowing the ML system to consume data as required. Depending on the preprocessor and training functionality, it may be possible to avoid storing the entire dataset in memory, making it possible to train some types of models on datasets that exceed system memory.
* **Archiving:** Many RDBMSs are able to create files from which the database can be restored, enabling snapshots of the database to be created. Snapshots mitigate the risk of catastrophic data loss. They also enable the exact state of the database to be captured for the purpose of reproducing model training or performing diagnostic tasks.
* **Logging:** Operations performed by users may be logged by the RDBMS, enabling the causes of harm to data integrity to be tracked down and the effects potentially reversed.

Although ML systems are certainly not the only systems that depend on valid data present in large volumes, they are somewhat unique amongst software systems in that the quality of the data they have access to strongly influences the value of their functionality and its output. As such, an essential priority of ML systems must be to ensure data integrity. RDBMSs are the most mature, widely proven technology created thus far for maintaining data integrity in the general case, and should be preferred over less principled data storage techniques such as files and archives.

## Adding Useful Metadata

Structure is a form of metadata that enables data to be reasoned about without regard to the values it contains. Database systems exploit this information to perform many of the actions discussed in the previous section. Similarly, metadata can be added to domain data to enable ML systems to perform various actions useful for ML tasks. Such metadata takes the form of fields (annotations), rather than structure, and includes the following:

* **_Identifier:_** Every element of data stored by the system must have a unique identifier so that it can be tracked as it is processed by the system. Aim to generate identifiers that uniquely and unambiguoustly identify data, but from which no insight into the data can be gained to avoid unintentional data dependencies. Universally unique identifiers (UUIDs) are suitable for this purpose.
* **_Origin:_** All data originates somewhere. The purpose of this piece of metadata is to identify the origin of data in a concise manner. Creating unique identifiers for origins is suggested for consistency across all origin values and to avoid the complexities of using the identifiers of origins from their respective domains (e.g. avoid the names of organisations and individuals). The number of origins is generally far lower than the number of data elements, so consider creating identifiers that make sense to the project team rather than less communicative identifiers such as those generated using UUID processes.
* **_Source:_** This piece of metadata identifies how data entered the system in a technical sense. It may identify, for example, an external system that is streaming data from a specific origin into the ML system. Like the _origin_ field, it is suggested to generate unique identifiers that are meaningful as there will typically be a relatively small number of sources. Be careful to identify sources in such a way that different versions of the source software are identified differently to ensure that irregularities in data caused by changes between versions can be detected and investigated.
* **_Timestamp:_** The primary reason for storing a timestamp is to track when data entered the ML system. This is useful for administrative purposes; to identify data that has significantly 'aged' or data that was affected by a temporary occurrence (e.g. consider learning of the existence of a defect and being able to determine the period in which the issue affected the data stored by the system).
* **_Attributes:_** In supervised machine learning, the label of an element of the training data indicates something we know about the data and intend the model to learn so that it can provide such information on other examples (e.g. an email might be labelled as _spam_ to support training a model to identify whether other such emails are unsolicited). A single element of domain data may have may have many associated labels (attributes), thus this piece of metadata may be a more complicated structure than a single label value. Alternatively, a separate element of metadata, with an appropriate name, can be used for each attribute the data possesses. In unsupervised training, attribute metadata can be omitted or ignored.
* **_Notes:_** This piece of metadata is useful to track any additional details about elements of data that are too specific, uncommon or disparate to justify representing using a dedicated piece of metadata. Types such as variable length strings or JSON are useful for this field, as such types contain human-readable values and also account for the possible variable length of such information.

Each element of domain data should have an identifier, origin, source, timestamp, attributes and, optionally, notes in addition to the data itself.

The primary use of the above metadata is diagnostic. The _origin_, _source_ and _timestamp_ fields, for example, enable data irregularities to be discovered according to origin, source and time period, respectively. Similarly, the _identifier_ field provides a means to track problematic elements of data, as well as tracking the data that was used to train a model. The _attributes_ field is not diagnostic. In the _Creating Datasets_ section of this post, the ability to exploit this field to prepare datasets will be explored. Whether the _notes_ field is diagnostic depends on its content. In contrast to the other fields, it exists primarily to ensure that secondary information associated with an element of data is not lost. Avoid confusing metadata with data, even when they apparently serve the same purpose. Do not use domain identifiers as metadata identifiers, since the ML system cannot guarantee that domain identifiers are unique. Do not use domain-specific timestamp information as the metadata timestamp, as this destroys the ability to trace defects known to be present over a particular period but not specific to a single origin or source.

As for the domain data itself, this can be stored using a collection of relevant fields. Alternatively, a single field can be used. This is useful if the data is received from the origin in an encrypted form and the intention is for the data to remain encrypted until it is processed by the system. This is also useful if the intention is to keep the element of data in the form it appears within its domain (or as close as possible), rather than destructuring that form and losing any information the form may itself contain. Although the previous section emphasises the general importance of structured data, the structure of domain data (the data on which models are trained and perform inference) is more important in its in-memory representation rather than its storage, as will be explored in the next post in this series, although a strong and rich representation of domain data should exist in the database where possible. For most other tasks performed by the ML system, a structure consisting of the metadata fields described in this section and some means of storing domain data are often sufficient.

## Introducing Data to the System

As noted in the previous section, all data has a source through which it enters the ML system. Although sources may seem valuable because they provide the data that makes training possible, they in fact present a substantial risk because they are not bound by the system's rules and constraints; for example, they may provide data with nonsenical attributes and timestamps that are outside valid ranges. Within the ML system the single source of data is the database, thus it is critical that the data it stores is valid and in a useful form. For this reason, a filter that the owners of the ML system have control over must exist between the database and each source. Each filter acts as a gatekeeper to the database, rejecting data that does not conform to the constraints of the ML system.

Although filters may differ depending on the source they are designed to be used with, they all perform the same tasks:

* **Validation:** This is the task of ensuring that all values in the provided data conform to the rules of the domain in which the ML system operates. Care must be taken to ensure that every value is of the expected type (e.g. ages are integers and names are strings) and within the constaints of how such values are represented within the system (e.g. ages are positive integers that cannot exceed a sensible maximum value and names are strings that contain fewer than 256 characters).
* **Sanitisation:** Data can contain identifying details that are undesirable to store in the database; consider, for example, confidentiality and privacy risks caused by users of the ML system having access to databases populated with personal and identifying information about individuals, and models unintentionally exposing personally identifiable information in inference results. Where possible, the filters to the database can be made to remove this information, avoiding the risk that such details will be accidentally exposed through the system's outputs.
* **Normalisation:** In the domain, data that has a singular logical structure may appear in various forms. For example, different types of telemetry devices may measure the same physical quantity but produce outputs that differ slightly in structure. Filters provide an opportunity to adapt the structure in which sources provide data into a different structure that is consistent across all data that enters the system.

Due to the nature of the task, much of the functionality across a system's filters may be identical. For this reason, it may be desirable to implement a single filter with different interfaces (e.g. a service with an endpoint for each source) or to consolidate the shared functionality in a library and implement separate filters for each source.

The intention is to obtain, from each source, clean and valid data that is as close to its original form as practical. Ensure that the filters are exhaustive in ensuring that any data that reaches the database is suitable for use in model training. Avoid changing data after it enters the database, as this undermines the utility of data tracing mechanisms. Also avoid using filters to generate missing values in data. Introducing missing values, such as defaults or via interpolation, introduces the risk that model performance will be influenced by data with no basis in reality. Such values may be impossible to distinguish from actual values within the database, and thus impossible to expunge from the database once they are introduced. Finally, all data that enters the system should be treated with suspicion. Even data that appears valid in isolation may contain undesirable patterns due to an unintentional defect or malicious actor. As such, the data within the database should be regularly audited to test its correctness and quality. The metadata fields presented in the previous section are useful for isolating the origin or source that is responsible for any such patterns.

## Avoid Storing Derived Data

Derived data is any data calculated using only data that is already available to the system. For example, if a system stores the date of birth and date of death of an individual, then that individual's age at the time of death can be readily calculated. In ML systems, derived data often takes the form of _features_ - representations of the domain data intended to improve the quality of the model trained on that data in one or more respects. In the ML community, the storage and distribution of dataset features, rather than the original dataset, is common (in part due to the computational cost of computing certain features). However, in the software engineering community, derived data is generally not stored and is instead calculated as required. This is partly due to the importance of access to domain data in its original form, but also because derived data is intractable from the process through which it was produced. Any change to that process can cause the expected derived data to differ from the actual derived data, in which case stored derived data may not reflect the state of the system's functionality. This makes it difficult to reason about the behaviour of the system and trace the data involved in model training. Unfortunately, attempting to ensure that all derived data is synchronised with the corresponding functionalities of the system is impractical for a number of reasons.

First, changes to the system can affect the process that calculates derived data even if that process is not directly changed. For example, changing thresholds for outlier filtering performed before a statistic is calculated may affect the derived data (the statistic), regardless of whether the statistic functionality is changed. Recalculating (and persisting) derived data only when the corresponding functionality changes does not guarantee that the system and stored derived data are in agreement, and recalculating (and persisting) derived data after every change to the system to ensure agreement is time-consuming and, in most cases, wasteful. Second, there must be a clean transition between the use of the old and new derived data. If this transition is poorly managed - for example, if derived datastores are not versioned - then it may not be possible to determine which derived data was used during all model training that occurred during the transition. Even if this transition is well-managed, the risk that derived data will not be updated due to developer error still exists. The risks of storing derived data are further compounded by the difficulty of identifying stale derived data. Stale data may not be obviously outdated, and the only means to verify whether derived data is outdated is to re-derive it.

The particular risk of stale derived data for ML systems is that a trained model is a product of the training parameters and the training dataset. If the dataset contains stale derived data, then the model is tainted by training on partially outdated or incorrect data and may produce unusual or erroneous inferences as a result. Attempting to investigate such issues may not be possible if the derived data involved in training is modified (on disk), or if the derived datastore contains stale derived data hidden amongst up-to-date derived data. Storing derived data is appealing, in the ML field, because feature extraction processes can be time-consuming and avoiding the recalculation of features implies that time can be saved. However, whether saving that time justifies the sacrifice of data and model traceability should be deeply considered. It is possible that more time will be wasted attempting to isolate issues caused by non-determinism in the training process than will be saved by storing derived data.

If the choice to store derived data within the system is made, consider whether the calculation of derived data can be performed within the system's filters and whether that derived data can be treated as domain data from the perspective of the system. That is to say, make the correctness of the derived data the responsibility of the functionality attempting to introduce it into the system. The advantages of this approach are two-fold: 1) the complexities of managing derived data within the ML system are avoided, and 2) the _source_ field in the data's metadata (see the _Adding Useful Metadata_ section) can be used to trace how stale derived data has entered the system when this inevitably occurs.

## Creating Datasets

Once a large set of clean and valid data exists it can be collected and partitioned in a vast number of ways to produce different datasets to use in model training. This can be achieved using SQL to select and filter data based on its metadata. This query, for example:

```sql
SELECT identifier, contents, attribute
FROM domain_data
WHERE source = 'queue'
AND timestamp >= '2017-01-01'
AND timestamp < '2018-01-01';
```

extracts the identifiers, values and attribute (from the `identifier`, `contents` and `attribute` fields, respectively) of all domain data (stored in the `domain_data` table of the database) which entered the system through a source identified as `queue` in the year of 2017. The metadata outlined in the section _Adding Useful Metadata_ enables the creation of many queries that select data based on its origin, source, timestamp and attributes without the need to inspect the domain data itself.

Queries based on metadata alone should be sufficient to produce most training datasets. If the need arises to select domain data based on some characteristic of the data itself, then it should be considered whether that characteristic should be encoded in the data's _attributes_ metadata field and selected on that basis. Discouraging queries that depend directly on domain data, rather than its metadata, is useful if the domain data is only partially structured within the database (if, as noted in _Adding Useful Metadata_ the intention is to store the domain data in a form as close to its representation in the domain as possible). Querying data that is only partially structured within a database is problematic for a number of reasons: it requires knowledge of the data's implicit structure; it is more error-prone since queries cannot be validated against the data's structure; and complexities such as the potential for missing aspects of the implicit structure must be accounted for. Furthermore, RDBMSs are less efficient at selectively querying semi-structured data, which is an important factor when the dataset is large. Avoid such issues by moving as many query-relevant aspects of domain data into metadata as possible.

Although metadata is useful for querying domain data, avoid creating queries that return any metadata other than the data's identifier and the attributes of interest. By limiting the amount of metadata available to the functionality that performs model training, the risks of metadata affecting model training - either by forming part of the dataset on which the model is trained or influencing how domain data is prepared before training occurs - are mitigated. The identifiers of domain data should be used only to trace that data as it passes through the system, and the attributes of interest should only be used to guide model training on the domain data (in supervised learning). In the context of model training, all other uses of metadata should be considered data dependency risks. In the next blog post in this series, strategies for preparing domain data for training will be explored.

## Maintain a Single Source of Truth

Effective data management requires vigilance to ensure that only valid data enters the system and discipline to ensure that the integrity of that data is maintained. In ML sytems, where the quality of data determines the usefulness of the models the system produces, the principles of data management and the practices described in this blog post must not be compromised, lest models are trained with tainted data and end-to-end control of model training is lost. Data integrity, and mechanisms to manage data integrity risks, must be a priority in all ML Xsystems.

