# Managing Model Training and Inference

## Managing the Complexity of Model Training

Preparing data to be processed by model functionality is a complicated task, as the previous posts in this series have explored. But it is generally a manageable task: it often consists of discrete transformations applied to data on a per-element basis. Model training presents an additional challenge because it involves transformations on collections of data, extracting information from the entire training dataset and embedding it into a single structure. The ability to trace aspects of the model directly to the elements of data that shaped those aspects is lost, and thus ends the extent to which data can be traced at it moves through the system. At this point, the task of model traceability begins. This blog post will demonstrate the importance of explicit control over how models are trained, and  provide strategies as to how this can be achieved, for the purpose of identifying, analysing and reproducing models.

## Organising Model Functionality

ML techniques consist of training and inference algorithms, the implementations of which form the core of the training and inference functionality of the ML system for the ML technique of choice. The system presents interfaces for both training and inference, which are implemented using both task-specific and shared functionality. The training interface receives a collection of domain data which constitutes the training dataset and returns a data structure that encodes the trained model. The inference interface returns inference results for each element of domain data it receives. Within the inference functionality is a trained model from which the inference results are obtained.

The precise organisation of components within ML systems' model functionality may vary according to the ML technique the system implements. In some cases there may be much functionality shared between the training and inference implementations in the form of operations common to both features. Or the features may be somewhat disparate, connected only by the fact that the inference functionality takes the output of the training functionality (the serialised model) as input. Some inference algorithms also have parameters that can be configured as desired, in which case this separate configuration must be supported by the system. The implementation of training and inference functionality should be guided by the design and requirements of the respective algorithms and the desired qualities of the software system (ideally testability and reproducibility for the purposes of isolating and eliminating errors).

How the model training process is utilised by the broader ML system can have consequences for model traceability - the ability to identify specific trained models, how they were trained, and with what data. Traceability can be achieved by introducing an additional input and output to the model training process: the model definition and training metadata, respectively. The model definition artefact serves as the configuration of the entire training process, including how data is prepared for training and the arguments to be provided for the parameters of the system's training functionality. The training metadata artefact contains data collected during training, including information about the training dataset and, where possible, details about the behaviour of the newly-trained model. Model definitions and training metadata enable the behaviour of the system to be defined and examined, respectively, and thus form a closed loop through which the training process can be inspected. They are the focus of the following two sections.

## Definition-based Training

A model definition is simply the collection of values that define the state of the system before training occurs. It is a form of configuration, but specific to model training. A model definition specifies what components of the system's data preparation and model training functionality are to be used to train the model the definition defines, as well as the parameters to be passed to those components to specify their behaviour during the training of that model. Model definitions are most useful when they are able to communicate, through their structure and the configuration values they contain, how a model should be trained independently of the implementation of training. In this way they serve as a statement of what the model produced by the system should be, and pairing serialised models with their definitions enables them to be unambiguously identified long after training occurs.

However, care must be taken to ensure that the concerns of preparing and executing the training components  - this includes both data preparation and model training functionality - of the system are separated. The configuration process proceeds as follows:

1. All details of data preparation and model training are captured in the model definition.
2. The training components of the system are configured according to the model definition.
3. The training dataset is processed by the configured components to produce the trained model and training metadata.

Crucially, the configuration of all training components occurs before processing begins on the dataset. This requires the implementation of a conceptual configuration layer in which each training component is configured as required. The configuration functionality of the system should be collectively viewed as a function that returns the specific training pipeline - a sequence of data preparation and model training components through which data passes when training occurs - defined by the model definition that it receives. This training pipeline expects a dataset as input and returns the trained model and training metadata as output. Represented in pseudo-code:

```python
# Load domain data into the process.
dataset = ...

# Specify all details of data preparation and model training.
model_definition = {...}

# Obtain a training pipeline configured according to the definition.
pipeline = configured_pipeline(model_definition)

# Execute the training pipeline on the training dataset.
trained_model, training_metadata = pipeline(dataset)
```

Using this approach, the concerns between configuration and execution are separated and can be evaluated and tested in isolation. If the functionality that generates pipelines from model definitions is deterministic (and it should be), then it can be quickly determined whether defects reside in the configuration or training functionality and, in the latter case, the problematic pipeline can be reproduced to assist further investigation.

The existence of model definitions also provides a number of additional advantages not strictly related to model identification and training but nevertheless useful to the creation of ML systems. These include:

* **Pipeline validation:** Configuring the entire pipeline in one step provides an opportunity to validate whether each component in the pipeline is able to consume the output of the preceding component, and thus whether the training process can actually be completed, before training begins. This is particularly useful given the time that may be required for data preparation and model training tasks.
* **Automated training:** Concentrating training details into a single configuration object and implementing a robust configuration layer in the ML system enables the full scope of configuration-controlled training functionality to be controlled through model definitions alone. Model training can be automated by extending the system to consume model definitions from some logical store (such as a database or queue), no additional configuration required.
* **Definition generation:** Model definitions, in consisting primarily of parameter values and associated metadata, are generally simple data objects. As such, it is usually trivial to create functionality for generating model definitions programmatically. Aside from further automating training, this is also useful for generating a wide variety of definitions for the purpose of exploring the training parameter space.
* **Interface stability:** Isolating the configuration of model training from training itself reduces the risk that any changes to the training functionality will require changes to the training interface through which users interact with the system. The impact of such changes may be contained to the configuration functionality, and the behaviour of the original and the revised systems can be compared using the same model definitons.
* **Model aliasing:** Model definitions are an essential tool for identifying trained models, but are not necessarily ergonomic for this purpose due to their verbosity. Deterministically hashing model definitions can be useful to generate a short identifier for the corresponding models. This also has the advantage of flagging potentially identical model definitions: if two model definition hashes match, then they most likely describe the same model (assuming deterministic training).

The above features are possible due to the separation between what a model should be - the definition - and what the model is - the serialised model. If the system's data preparation and model training functionality is deterministic, then model definitions can generally be treated as proxies for the model they define. However, definitions are only one aspect of identifying models: models trained using the same definition differ if trained on different datasets and, as such, can only be fully identified if details about the training dataset and other training factors are also captured. Theses details are the focus of the next section.

##  Capturing Training Metadata 

Tracking the data actually used in model training is complicated for a number of reasons, even if the principles of data management discussed in the second post in this series are followed. First, domain data is stored on disk and thus attempts to programmatically recreate the same dataset are, in the general case, impossible; consider, for example, how the same data retrieval query can be made to produce different results by modifying the contents of the database. Second, elements of the training dataset may be discarded by the system's data preparation functionality in filtering steps, thus the set of domain data ultimately involved in model training may not match the set initially provided to the training process. Similarly, the form of the data that reaches model training depends on the state of the system's data preparation functionality when training occurs; the same domain data may reach the system's model training functionality in different forms depending on the version of the system performing the training.

Even assuming that domain data may not arbitrarily change on disk, anticipating what domain data will reach the system's model training functionality and in what form is impractical for the stated reasons. In light of this ambiguity, the only course of action is to capture data and process details, training metadata, that provide insight into training as it occurs. In the naive case, if both a unique identifier for the software that performed training and a copy of the initial training dataset are captured, then the exact conditions of training can be recreated (in conjunction with the model definition). In practice, the software that performs training may depend on third-party libraries and other details of the execution environment that may be difficult to directly, unambiguously and exhaustively capture, and the volume of data used in training may make storing a copy of this dataset alongside the trained model impractical. The more exhaustive the capturing of training metadata, the greater the technical burden of this task on the system.

There are, however, strategies for capturing concise metadata that is sufficient both for determining whether models were trained under identical conditions (assuming they share the same model definition) and for recognising whether the training functionality or training dataset differ when attempting to reproduce model training. These include:

* **Software version:** Software versioning systems enable revisions of the system to be uniquely identified succinctly using commit hashes. Capture the hash of the commit with which training is performed so that the precise version of the system is known. Extend the utility of this metadata by making the system programmatically configure the environment in which training takes place, thereby avoiding the need to separately capture these details.
* **Dataset identifier:** The second and third blog posts in this series outlined the advantages that uniquely identifying each element of domain data can provide. Leverage those identifiers to derive an identifier for the dataset provided to the training process that reflects the data it contains. A deterministic hash of the lexicographically sorted identifiers of the domain data that makes up the dataset is suitable for this task. Such identifiers can indicate whether the datasets used in separate training processes are identical.

Both pieces of metadata are straightforward to obtain and trivially small (in terms of bytes). They do not enable the conditions of training to be recreated, but do indicate whether different training processes are comparable.

Various statistics and computed values are also useful to capture before training completes and the dataset is discarded. This metadata can assist the discovery of unintentional biases in the composition of the dataset, which may result in the creation of models that do not perform as intended:

* **Derived calibration values**: Any values derived from the training dataset for the purpose of calibrating the feature extraction operations within the system's data preparation functionality should be captured to assist understanding how the composition of the dataset affected subsequent feature extraction actions. The separation of feature calibration from feature extraction discussed in the second post in this series simplifies capturing this metadata.


* **Distribution of attributes:** The distribution of attributes amongst elements in the dataset provides insight into the characteristics of the data used in training and can be calculated at little computational cost. This information is useful for diagnostic tasks by indicating whether the relative proportion of attributes across the dataset - for example, the absence of examples with a particular attribute - is likely to have influenced model training.
* **Domain specific details:** Characteristics of the training dataset relevant to the domain may provide insight into how the model will perform when it enters use in the production system. For example, if a spam classifier is primarily trained on short emails then it may be more effective at discerning whether short emails are solicited or unsolicited than long emails. The information relevant to capture in such metadata will vary by domain and may only become apparent from careful evaluation of the performance of previously trained models.

Note that this information must be captured after the system's data preparation functionality has discarded the elements of the dataset determined to be unsuitable for training, since the statistics of the original dataset may not match subset of domain data on which training actually occurs. After training, the metadata should be studied to ensure that there is no evidence of undesirable biases that may influence the performance of the model. The influence of such biases, if not captured at the time of training, may only become apparent as biases in the inference results produced by the model in the staging and production environments.

## Evaluating Model Performance

In supervised learning, elements of the training dataset are paired with labels or attributes that indicate the information that the model is intended to learn. The existence of labelled data enables the performance of the model to be evaluated as part of the training process. This typically occurs as follows:

1. Prior to training, the training dataset is partitioned into a larger training subset and smaller testing subset.
2. Training is performed using only the training subset and corresponding labels.
3. Inference is performed using the trained model on each element of the test subset.
4. The inference results are compared to the labels associated with the elements of the test subset and statistics are calculated.

A key result of model evaluation is determining for what proportion of the test subset the model infers the label known to be associated with the data, known as the accuracy of the model. A high accuracy indicates that the training process correctly extracts the desired information from domain data and embeds it in the trained model. A low accuracy may indicate that the design of the model is flawed, the dataset was poorly chosen, or that a defect exists in the system's training functionality. Model accuracy and similar statistics provide insight into model performance, which is essential for deciding whether a model should be used in production.

In terms of the operation of the system, supervised learning requires the use of both training and inference functionality: the training functionality first produces the model using the training subset, then the inference functionality uses the model to perform inference on the test subset. The incorporation of inference into the training process has a number of implications for the model definition:

* Model definitions must contain sufficient information to enable the training dataset to be partitioned into the training and testing subsets in the desired proportion, and to enable the distribution of labels in both subsets to be precisely specified. This enables the distribution of labels in the subsets to be made as close as possible to their distribution in the domain, ensuring that the statistics collected during model evaluation are as indicative as possible of how the model is likely to perform in the production environment.
* Model definitions must be extended to contain any parameters required by the system's model inference functionality when evaluation is performed. This is important not only because of the definition's role as a configuration artefact, but also because knowledge of how evaluation was performed helps to understand factors affecting the calculated accuracy value.

The system must be also extended to include functionality for partitioning datasets to enable the creation of the training and testing subsets. Note that the distribution of labels within the subsets must be equal, otherwise the calculated model accuracy may not be representative of the proportional training effort for each label.

While the test subset is available to the system during model evaluation, it is useful to collect statistics aside from accuracy to provide the user insight into the performance of the trained model. Four such statistics are the number of true positives, true negatives, false positives and false negatives. This information can indicate how subsequent training should be modified to increase model accuracy and enables other common statistics such as precision, recall, the F1 score and Matthews correlation coefficient to be calculated. Such statistics provide insight into an aspect of model performance that cannot be captured using model accuracy alone. Unfortunately, many statistics are only useful or applicable under certain conditions, thus the set of statistics that can be collected during model evaluation in the general case is limited.

## Managing Trained Models

The trained models produced using most ML techniques are not particularly informative to humans, at least in their serialised form, and the effect of the data embedded in the trained model on inference results is not transparent. If details regarding how the model was trained and, in the supervised learning case, how it performed during evaluation are lost, then such information can only be guessed at based on the performance of the model on available datasets. To minimise this risk, models must be stored alongside their model definition (capturing the configuration of the system's training functionality during training), training metadata (capturing the state of the dataset and functionality at the time of training), and evaluation results (capturing the performance of the model under the same conditions in which training occurred). This information assists understanding why and how a model was created and whether the conditions of training were useful for the problem the model was intended to address.

However, storing such information is useless if the model is able to be modified after training is completed. This is a risk when, for example, additional training is performed on a model after the initial training (this is possible in some ML techniques). Unless the model definition, training metadata and evaluation results of the second training process are captured in addition to those of the original process, the performance of the model may be difficult to predict and the ability to recreate the model is lost. The mutation of models may also occur simply as a result of defects in the system or cycles of lossy serialisation and deserialisation. In particular, mutations that occur due to precision loss in the serialisation functionality can be extremely difficult to identify since their impact on the performance of trained models may be subtle.

For this reason it is suggested to design training and inference functionality in such a way that: 1) model serialisation is discouraged at any time other than the conclusion of training and 2) the functionality that deserialises trained models produces a representation of the model that is not serialisable (or has no corresponding serialisation functionality). By intentionally disrupting the serialisation-deserialisation loop, the frequency at which mutated models will be accidentally persisted should be reduced. Similarly, the deserialisation, mutation and re-serialisation of model definitions, training metadata and evaluation results must be discouraged to avoid information loss and wasted effort caused by flawed metadata.

## ML Systems are Software Systems

Despite the inscrutability at the core of ML systems, they remain software systems and thus benefit from many of the principles of sound software engineering - principles such as:

* The principled management of data and consistent metadata collection.
* The validation, sanitisation and normalisation of data that enters the system.
* The avoidance of storing data that can be derived from data already in the system.
* The separation of responsibilities amongst functionality that serves different purposes.
* The creation of rich data representations that facilitate access to specific aspects of the data.
* The creation of functionality that is total and free from side-effects and non-determinism.
* The appropriate level of configurability of the system's functionality.
* The capture of information during complex processes for diagnostic purposes.
* The preference for immutability to reduce the risk of unexpected system behaviour.

If ML systems differ in any way from other types of software systems, it is in the way their core feature is both highly susceptible to subtle defects and immensely difficult to inspect. The complicated process by which models are created provides many opportunities for error, and the nature of the process provides many opportunties for errors to avoid discovery. It is for this reason that the stated principles are so important in the creation of ML systems. This series of blog posts has outlined how specific risks to systems that perform ML tasks can be mitigated using specific design strategies based on these principles.

